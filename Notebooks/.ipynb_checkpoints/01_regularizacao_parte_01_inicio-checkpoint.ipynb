{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularização é uma técnica utilizada em machine learning para prevenir overfitting, que ocorre quando um modelo se ajusta excessivamente aos dados de treinamento, capturando não apenas os padrões relevantes, mas também o ruído e as peculiaridades específicas desses dados. O objetivo da regularização é impor uma penalização para complexidade excessiva do modelo, incentivando-o a ser mais simples e, portanto, mais capaz de generalizar para novos dados.\n",
    "\n",
    "### Como Funciona a Regularização\n",
    "\n",
    "Regularização é geralmente implementada adicionando um termo de penalização à função de custo do modelo, que o modelo tenta minimizar durante o treinamento. Existem diferentes formas de regularização, sendo as mais comuns:\n",
    "\n",
    "1. **L1 Regularization (Lasso)**:\n",
    "   - Adiciona uma penalização proporcional à soma dos valores absolutos dos coeficientes do modelo.\n",
    "   - Promove a sparsity, ou seja, a esparsidade, forçando alguns coeficientes a serem exatamente zero, o que pode resultar em um modelo mais interpretável ao selecionar automaticamente features relevantes.\n",
    "\n",
    "2. **L2 Regularization (Ridge)**:\n",
    "   - Adiciona uma penalização proporcional à soma dos quadrados dos coeficientes do modelo.\n",
    "   - Reduz os coeficientes, mas dificilmente os torna exatamente zero, resultando em um modelo com todos os features ainda contribuintes, mas com impactos reduzidos.\n",
    "\n",
    "3. **Elastic Net**:\n",
    "   - Combina as penalizações L1 e L2, permitindo um equilíbrio entre a sparsity do Lasso e a suavização do Ridge.\n",
    "\n",
    "### Importância da Regularização\n",
    "\n",
    "1. **Prevenção do Overfitting**:\n",
    "   - Regularização é essencial para prevenir o overfitting, especialmente em modelos complexos com muitos parâmetros. Sem regularização, um modelo pode se ajustar tão bem aos dados de treinamento que perde a capacidade de generalizar para novos dados.\n",
    "\n",
    "2. **Melhor Generalização**:\n",
    "   - Ao penalizar a complexidade, a regularização incentiva o modelo a ser mais robusto, capturando apenas os padrões verdadeiros dos dados. Isso resulta em melhor desempenho em dados de validação e teste, ou seja, dados que o modelo nunca viu antes.\n",
    "\n",
    "3. **Simplicidade e Interpretabilidade**:\n",
    "   - Em muitos casos, modelos simples são preferíveis por serem mais fáceis de interpretar. Regularização, especialmente L1, pode ajudar a criar modelos mais simples e interpretáveis ao eliminar features irrelevantes.\n",
    "\n",
    "4. **Controle de Multicolinearidade**:\n",
    "   - Em modelos de regressão linear, a multicolinearidade entre as features pode tornar os coeficientes instáveis e difíceis de interpretar. A regularização ajuda a estabilizar esses coeficientes, reduzindo a variância do modelo e melhorando a robustez.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemplo simples - sem regularização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import Lasso, LinearRegression, Ridge\n",
    "from sklearn.model_selection import train_test_split, ValidationCurveDisplay\n",
    "\n",
    "from src.config import DADOS_EXEMPLO_REGULARIZACAO\n",
    "from src.graficos import plot_residuos\n",
    "\n",
    "sns.set_theme(palette=\"bright\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(DADOS_EXEMPLO_REGULARIZACAO)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usando todos os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separando em treino e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ridge Regression**, também conhecida como **Regressão L2**, é uma técnica de regularização aplicada a modelos de regressão linear. O principal objetivo do Ridge é melhorar a capacidade de generalização do modelo, especialmente em cenários onde as features estão altamente correlacionadas ou onde o número de features é grande em comparação com o número de observações.\n",
    "\n",
    "### Contexto da Regressão Linear\n",
    "\n",
    "Na regressão linear simples, buscamos encontrar um vetor de pesos $ \\mathbf{w} = [w_1, w_2, \\dots, w_p] $ que minimiza a soma dos erros quadrados entre as previsões do modelo e os valores reais do target $ y $. A função de custo típica da regressão linear é dada por:\n",
    "\n",
    "$$\n",
    "J(\\mathbf{w}) = \\sum_{i=1}^{n} \\left( y_i - \\hat{y}_i \\right)^2 \n",
    "$$\n",
    "\n",
    "onde:\n",
    "- $ n $ é o número de observações,\n",
    "- $ y_i $ é o valor real da i-ésima observação,\n",
    "- $ \\hat{y}_i $ é a previsão do modelo para a i-ésima observação,\n",
    "\n",
    "### Introdução da Regularização L2 (Ridge)\n",
    "\n",
    "O Ridge Regression modifica a função de custo da regressão linear para incluir um termo de penalização que depende da magnitude dos pesos $ \\mathbf{w} $. A função de custo regularizada em Ridge é dada por:\n",
    "\n",
    "$$\n",
    "J(\\mathbf{w}) = \\sum_{i=1}^{n} \\left( y_i - \\hat{y}_i \\right)^2 + \\alpha \\sum_{j=1}^{p} w_j^2\n",
    "$$\n",
    "\n",
    "onde:\n",
    "- $ \\alpha $ é o hiperparâmetro de regularização que controla a força da penalização,\n",
    "- $ \\sum_{j=1}^{p} w_j^2 $ é a soma dos quadrados dos pesos.\n",
    "\n",
    "O termo de regularização $ \\alpha \\sum_{j=1}^{p} w_j^2 $ penaliza a função de custo original, incentivando o modelo a minimizar não apenas os erros de predição, mas também a magnitude dos pesos. Isso tem o efeito de:\n",
    "- **Reduzir a variância**: Modelos complexos com pesos grandes podem se ajustar muito bem aos dados de treinamento, mas falham em generalizar para novos dados. A regularização força esses pesos a serem menores, criando um modelo que é mais robusto a novos dados.\n",
    "- **Manter todas as features no modelo**: Diferentemente do Lasso, que pode reduzir alguns pesos a exatamente zero, eliminando features, o Ridge tende a manter todos os pesos não nulos, mas os diminui. Isso pode ser vantajoso em cenários onde todas as features são consideradas importantes, mas correlacionadas.\n",
    "\n",
    "### Escolha do Hiperparâmetro $ \\alpha $\n",
    "\n",
    "O hiperparâmetro $ \\alpha $ controla o trade-off entre o ajuste do modelo aos dados de treinamento e a penalização da magnitude dos pesos. Quando $ \\alpha = 0 $, a regularização é removida e o modelo se torna equivalente à regressão linear simples. À medida que $ \\alpha $ aumenta, a penalização se torna mais forte, resultando em pesos menores e, potencialmente, em um modelo menos suscetível ao overfitting.\n",
    "\n",
    "A escolha do valor de $ \\alpha $ é geralmente feita usando validação cruzada, onde diferentes valores de $ \\alpha $ são testados para encontrar aquele que proporciona o melhor equilíbrio entre bias e variância.\n",
    "\n",
    "### Vantagens do Ridge\n",
    "\n",
    "1. **Redução do Overfitting**: Ao penalizar pesos grandes, o Ridge reduz a complexidade do modelo, ajudando a prevenir o overfitting.\n",
    "2. **Estabilidade em Multicolinearidade**: Ridge é particularmente útil em situações onde as features são altamente correlacionadas, pois a regularização ajuda a estabilizar os coeficientes.\n",
    "\n",
    "### Limitações do Ridge\n",
    "\n",
    "1. **Todas as Features são Mantidas**: Como o Ridge tende a manter todos os coeficientes não nulos, ele não é útil se o objetivo é selecionar um subconjunto de features importantes.\n",
    "2. **Interpretação**: Em cenários onde a interpretabilidade dos coeficientes é crucial, a regularização pode dificultar a interpretação direta dos pesos, pois eles são enviesados para reduzir a magnitude.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lasso Regression** (Least Absolute Shrinkage and Selection Operator) é uma técnica de regularização aplicada a modelos de regressão linear, que visa melhorar a capacidade de generalização do modelo e, ao mesmo tempo, realizar a seleção de features. Lasso é especialmente útil em situações onde se deseja identificar e utilizar apenas as features mais relevantes, eliminando aquelas que têm pouca ou nenhuma contribuição para o modelo.\n",
    "\n",
    "### Contexto da Regressão Linear\n",
    "\n",
    "Na regressão linear simples, o objetivo é encontrar um vetor de pesos $ \\mathbf{w} = [w_1, w_2, \\dots, w_p] $ que minimiza a soma dos erros quadrados entre as previsões do modelo e os valores reais do target $ y $. A função de custo da regressão linear padrão é dada por:\n",
    "\n",
    "$$\n",
    "J(\\mathbf{w}) = \\sum_{i=1}^{n} \\left( y_i - \\hat{y}_i \\right)^2 \n",
    "$$\n",
    "\n",
    "onde:\n",
    "- $ n $ é o número de observações,\n",
    "- $ y_i $ é o valor real da i-ésima observação,\n",
    "- $ \\hat{y}_i $ é a previsão do modelo para a i-ésima observação,\n",
    "\n",
    "### Introdução da Regularização L1 (Lasso)\n",
    "\n",
    "Lasso Regression modifica a função de custo da regressão linear para incluir um termo de penalização que depende da soma dos valores absolutos dos pesos $ \\mathbf{w} $. A função de custo regularizada em Lasso é dada por:\n",
    "\n",
    "$$\n",
    "J(\\mathbf{w}) = \\sum_{i=1}^{n} \\left( y_i - \\hat{y}_i \\right)^2 + \\alpha \\sum_{j=1}^{p} |w_j|\n",
    "$$\n",
    "\n",
    "onde:\n",
    "- $ \\alpha $ é o hiperparâmetro de regularização que controla a força da penalização,\n",
    "- $ \\sum_{j=1}^{p} |w_j| $ é a soma dos valores absolutos dos pesos.\n",
    "\n",
    "### Como Lasso Funciona\n",
    "\n",
    "O termo de regularização $ \\alpha \\sum_{j=1}^{p} |w_j| $ adiciona uma penalização linear na função de custo original, o que incentiva o modelo a não apenas minimizar os erros de predição, mas também a reduzir o número de pesos não nulos. Isso resulta em:\n",
    "\n",
    "- **Seleção de Features**: O Lasso pode forçar alguns dos coeficientes $ w_j $ a serem exatamente zero, efetivamente eliminando as features correspondentes do modelo. Isso faz do Lasso uma ferramenta poderosa para seleção de features.\n",
    "  \n",
    "- **Redução da Variância**: Ao simplificar o modelo e eliminar features irrelevantes, o Lasso ajuda a reduzir a variância, o que melhora a capacidade do modelo de generalizar para novos dados.\n",
    "\n",
    "### Escolha do Hiperparâmetro $ \\alpha $\n",
    "\n",
    "O hiperparâmetro $ \\alpha $ controla o trade-off entre ajuste e regularização. Quando $ \\alpha = 0 $, o modelo se reduz à regressão linear padrão sem regularização. À medida que $ \\alpha $ aumenta, mais coeficientes são reduzidos a zero, resultando em um modelo mais simples e com menos features.\n",
    "\n",
    "A escolha de $ \\alpha $ é crítica e geralmente é feita usando técnicas de validação cruzada. Escolher um $ \\alpha $ adequado ajuda a equilibrar o modelo entre underfitting (quando $ \\alpha $ é muito grande e muitos coeficientes são zerados) e overfitting (quando $ \\alpha $ é muito pequeno e poucos coeficientes são penalizados).\n",
    "\n",
    "### Vantagens do Lasso\n",
    "\n",
    "1. **Seleção Automática de Features**: Uma das principais vantagens do Lasso é sua capacidade de selecionar automaticamente as features mais importantes ao forçar os coeficientes de features menos importantes a zero.\n",
    "\n",
    "2. **Interpretação Simplificada**: Como o Lasso tende a eliminar features irrelevantes, o modelo resultante é mais simples e fácil de interpretar.\n",
    "\n",
    "3. **Robustez contra Multicolinearidade**: Lasso pode ser particularmente útil em cenários com multicolinearidade (onde as features são altamente correlacionadas), pois ele seleciona uma ou poucas features entre as correlacionadas e zera as outras.\n",
    "\n",
    "### Limitações do Lasso\n",
    "\n",
    "1. **Número de Features Selecionadas**: Se houver muitas features correlacionadas, o Lasso pode selecionar apenas uma delas, ignorando outras que podem ser igualmente importantes.\n",
    "\n",
    "2. **Cenários com Muitas Features e Poucos Dados**: Em situações onde o número de features é muito grande em comparação com o número de observações, o Lasso pode eliminar muitas features, potencialmente ignorando informações úteis.\n",
    "\n",
    "3. **Bias dos Coeficientes**: Como o Lasso penaliza todos os coeficientes, ele introduz um viés no modelo. Isso pode levar a uma subestimação dos verdadeiros valores dos coeficientes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning",
   "language": "python",
   "name": "machine_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
